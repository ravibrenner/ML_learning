---
title: "tidymodels"
author: "Ravi Brenner"
date: "2025-06-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

Following along with the tidymodels tutorial found here: https://www.tidymodels.org/start/models/

I have a lot of R experience, and some experience with caret and tidymodels too, so hopefully this will be easy for me.

# Build a model
```{r}
library(tidymodels)
library(readr)       # for importing data
library(broom.mixed) # for converting bayesian models to tidy tibbles
library(dotwhisker)  # for visualizing regression results

theme_set(theme_bw())
```

Read in sea urchins data
```{r}
urchins <- read_csv("https://tidymodels.org/start/models/urchins.csv") |>
  setNames(c("food_regime","initial_volume","width")) |>
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High")))

head(urchins)
str(urchins)
```

We can quickly plot this data, since it isn't very large

```{r}
ggplot(urchins,
       aes(x = initial_volume, y = width , color = food_regime)) +
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  scale_color_viridis_d(option = "plasma", end = 0.7) 
```

So there's a generally positive relationship, and it appears to be impacted by food_regime

## fitting a simple model

Creating the model in tidymodels means creating a pipeline of components

```{r}
lm_mod <- linear_reg() 
```

```{r}
lm_fit <- lm_mod |>
  fit(width ~ initial_volume * food_regime, data = urchins)

tidy(lm_fit)
```

```{r}
tidy(lm_fit) |>
  dotwhisker::dwplot(dot_args = list(size = 2,color = "black"),
                     whisker_args = list(color = "black"),
                     vline = geom_vline(xintercept = 0, color = "gray",linetype = 2))
```

So we very easily fit a model. `fit` from parsnip is clearly very similar to `train` from caret

## Predicting with the model

```{r}
new_points <- expand_grid(initial_volume = 20, 
                          food_regime = c("Initial", "Low", "High"))

mean_pred <- predict(lm_fit, new_data = new_points)
mean_pred

conf_int_pred <- predict(lm_fit, new_data = new_points,
                         type = "conf_int")
conf_int_pred

plot_data <- new_points |>
  bind_cols(mean_pred) |>
  bind_cols(conf_int_pred)
```

```{r}
ggplot(plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "urchin size")
```

Very simple, and familiar from working with caret.

## use a different engine

We can train more complicated models using other engines. here is a stan_glm model

```{r}
prior_dist <- rstanarm::student_t(df = 1)

set.seed(123)

# creating the model itself
bayes_mod <- linear_reg() |>
  set_engine("stan",
             prior_intercept = prior_dist,
             prior = prior_dist)

bayes_fit <- bayes_mod |>
  fit(width ~ initial_volume * food_regime, data = urchins)
```

```{r}
tidy(bayes_fit, conf.int = T)
```

We can now recycle our code from earlier

```{r}
bayes_plot_data <- new_points |>
  bind_cols(predict(bayes_fit, new_data = new_points)) |>
  bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))
```

```{r}
ggplot(bayes_plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "urchin size") + 
  ggtitle("Bayesian model with t(1) prior dist")
```

# Preprocess data with recipes
```{r}
library(nycflights13)
library(skimr)
```

Import the data
```{r}
set.seed(123)

flights_data <- flights |>
  mutate(arr_delay = if_else(arr_delay >= 30, "late","on_time"),
         arr_delay = factor(arr_delay),
         date = lubridate::as_date(time_hour)) |>
  inner_join(weather, by = join_by("origin","time_hour")) |>
  select(dep_time, flight, origin, dest, air_time,
         distance, carrier, date, arr_delay, time_hour) |>
  na.omit() |>
  mutate(across(where(is.character), as.factor))
```

check how many flights arrived late
```{r}
flights_data |>
  count(arr_delay) |>
  mutate(pct = n / sum(n))
```

check out the data using glimpse (like `str()`)
```{r}
glimpse(flights_data)
```

note that arr_delay is a binary outcome variable, and that flight and time_hour are unique to each row.

```{r}
flights_data |>
  skim(dest,carrier)
```
104 destinations and 16 carriers, which are large numbers for categorical variables. may need to deal with that later.

data splitting
```{r}
set.seed(222)

data_split <- initial_split(flights_data, prop = 0.75)

train_data <- training(data_split)
test_data <- testing(data_split)
```

## creating the recipe

For a simple glm model. The recipe function takes a formula and the data. then, you add roles to the recipe, specifying what each column should do in the model. in this case we specify the IDs, since they shouldn't really be included in the model
```{r}
flights_rec <- recipe(arr_delay ~ ., data = train_data) |>
  update_role(flight, time_hour, new_role = "ID")
```

Now we want to create some features ("feature engineering"). We could just create new columns, and then select the ones we want in the model (e.g. using dplyr). however we can make it part of the recipe workflow to maintain the full data structure

Here we are doing some modifications to the date column. keep_original_cols = FALSE drops the old in favor of the new. we can also create dummy variables
```{r}
flights_rec <- recipe(arr_delay ~ ., data = train_data) |>
  update_role(flight, time_hour, new_role = "ID") |>
  step_date(date, features = c("dow","month")) |>            
  step_holiday(date, 
               holidays = timeDate::listHolidays("US"), 
               keep_original_cols = FALSE) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors()) # removes factor levels with a single value (zero variance = zv)
```

All that work has just written the recipe of what should occur, nothing has actually happened.

Now we define the model as before
```{r}
glm_mod <- logistic_reg() |>
  set_engine("glm")
```

Now create the workflow
```{r}
flights_wflow <- workflow() |>
  add_model(glm_mod) |>
  add_recipe(flights_rec)
```

Now fit the actual model
```{r}
flights_fit <- flights_wflow |>
  fit(data = train_data)
```

```{r}
flights_fit |>
  extract_fit_parsnip() |>
  tidy()
```

Now we can do predictions. As a reminder, the steps so far were
1. build model
2. create recipe
3. bundle in workflow
4. train workflow using fit

now we have 5. predict
```{r}
predict(flights_fit, test_data)
```

To get probabilities we could set type = "prob" or we could use augment like this:
```{r}
flights_pred <- flights_fit |>
  broom::augment(test_data) 

flights_pred |>
  select(arr_delay,time_hour, flight, .pred_class, .pred_on_time)
```

Finally, to assess performance we can look at the ROC curve
```{r}
flights_pred |>
  roc_curve(truth = arr_delay, .pred_late) |>
  autoplot()

flights_pred |>
  roc_auc(truth = arr_delay, .pred_late)
```

In principle, the recipe/workflow combination earlier should make it easier to try out different models or features. for example we could fit the unmodified data like this:

```{r}
flights_wflow2 <- workflow() |>
  add_model(glm_mod) |>
  add_formula(arr_delay ~ .)

flights_fit2 <- flights_wflow2 |>
  fit(data = train_data |>
        select(-time_hour, -flight))

flights_fit2 |>
  extract_fit_parsnip() |>
  tidy()

flights_pred2 <- flights_fit2 |>
  broom::augment(test_data) 

flights_pred2 |>
  roc_auc(truth = arr_delay, .pred_late)
```


# Evaluate model with resamples
```{r}
library(modeldata)

data(cells, package = "modeldata")
cells
```

There are two values of class, PS for poorly segmented, and WS for well segmented

```{r}
cells |>
  count(class) |>
  mutate(pct = n / sum(n))
```

For data splitting, in this case it has been done for us, so that the proportions in each are consistent
```{r}
set.seed(123)
cell_split <- initial_split(cells |> select(-case),
                            strata = class)

cell_train <- training(cell_split)
cell_test <- testing(cell_split)

cell_train |> count(class) |> mutate(pct = n/sum(n))
cell_test |> count(class) |> mutate(pct = n/sum(n))
```

Here we'll use a random forest modeling approach using ranger. this is a somewhat more opaque modeling procedure than glms

```{r}
rf_mod <- rand_forest(trees = 1000) |>
  set_engine("ranger") |>
  set_mode("classification")
```

Now we can fit the model
```{r}
set.seed(234)
rf_fit <- rf_mod |>
  fit(class ~ ., data = cell_train)

rf_fit
```

we previously saw how to use roc_auc() to check model fit. We could also use accuracy() in theory.

Here it is on the training set
```{r}
rf_training_pred <- predict(rf_fit, cell_train) |>
  bind_cols(predict(rf_fit, cell_train, type = "prob")) |>
  bind_cols(cell_train |> select(class))

rf_training_pred |>
  roc_auc(truth = class, .pred_PS)

rf_training_pred |>
  accuracy(truth = class, .pred_class)
```

Those values are too good to be true. Our model is probably overfit, and there is a performance drop off with the testing data
```{r}
rf_testing_pred <- predict(rf_fit, cell_test) |>
  bind_cols(predict(rf_fit, cell_test, type = "prob")) |>
  bind_cols(cell_test |> select(class))

rf_testing_pred |>
  roc_auc(truth = class, .pred_PS)

rf_testing_pred |>
  accuracy(truth = class, .pred_class)
```

We skipped an important step here, which is resampling! Most commonly we use K fold cross validation.

```{r}
set.seed(345)
folds <- vfold_cv(cell_train, v = 10)
```

```{r}
rf_wf <- 
  workflow() |>
  add_model(rf_mod) |>
  add_formula(class ~ .)

set.seed(456)
rf_fit_rs <- 
  rf_wf |>
  fit_resamples(folds)
```

```{r}
collect_metrics(rf_fit_rs)
```


# Tune model parameters

```{r}
library(rpart.plot)
library(vip)
```

```{r}
data(cells, packages = "modeldata")
cells
```

Now we'll tune a random forest model but with more control/emphasis on the hyperparameters. In this case, cost complexity and max tree depth.

```{r}
set.seed(123)
cell_split <- initial_split(cells |> select(-case),
                            strata = class)
cell_train <- training(cell_split)
cell_test <- testing(cell_split)
```

The fact that we want to tune is encoded in the parsnip part of the modeling process, where we specify the model. tune() here is just a placeholder

```{r}
tune_spec <- decision_tree(cost_complexity = tune(),
                           tree_depth = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

Now we make the grid. grid_regular() sort of auto-selects some reasonable starting values, given the number of values I want
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
```

Finally set up the cross validation
```{r}
set.seed(234)
cell_folds <- vfold_cv(cell_train)
```

Now we tune using tune_grid(). In theory we could tune a model along with a recipe, or tune a workflow. here is the workflow version:

```{r}
set.seed(345)

tree_wf <- workflow() |>
  add_model(tune_spec) |> # the model from earlier
  add_formula(class ~ . ) # model formula

tree_res <- tree_wf |>
  tune_grid(resamples = cell_folds, # the data
            grid = tree_grid) # the tuning grid

tree_res |>
  collect_metrics() |>
  mutate(tree_depth = factor(tree_depth)) |>
  ggplot(aes(x = cost_complexity, y = mean, color = tree_depth)) + 
  geom_line() + 
  geom_point() + 
  facet_wrap(~.metric, scales = "free",nrow = 2) + 
  scale_x_log10(labels = scales::label_number()) + 
  scale_color_viridis_d(option = "plasma",begin = 0.9, end =0)
```

We could quickly check the best ones like this:
```{r}
tree_res |>
  show_best(metric = "roc_auc")

tree_res |>
  select_best(metric = "accuracy")
```

To get to the final model, we select those parameters
```{r}
final_wf <- 
  tree_wf |>
  finalize_workflow(tree_res |> select_best(metric = "accuracy"))
```

And do the fit

(note final fit uses full data, and evaluated on the testing data only)
```{r}
final_fit <- final_wf |>
  last_fit(cell_split)

final_fit |>
  collect_metrics()

final_fit |>
  collect_predictions() |>
  roc_curve(class,.pred_PS) |>
  autoplot()
```

At this stage, if you want to dive deeper into the final fit, you can extract the tree data from the workflow object/fitted object
```{r}
final_tree <- extract_workflow(final_fit)

final_tree |>
  extract_fit_engine() |>
  rpart.plot(roundint = F)
```

We could do variable importance with this too, anything we would do with the raw model output.
```{r}
final_tree |>
  extract_fit_parsnip() |>
  vip()
```

Note, we just used some default values for the hyperparameters here. there are more hyperparameters, and we could have specified a wider or more sepcific grid. grid_regular() just returns a tibble, so any tibble we want to construct (e.g. using expand_grid) would work too.

# Case study
```{r}
hotels <- read_csv("https://tidymodels.org/start/case-study/hotels.csv") |>
  mutate(across(where(is.character), as.factor))

glimpse(hotels)
```

The goal is to build a model to predict which stays included children/babies, and which did not--the outcome variable being "children"
```{r}
hotels |>
  count(children) |>
  mutate(pct = n/sum(n))
```

The percentage is relatively small, which could lead to some overfitting issues down the road. For this case study, we will leave it as-is.

## splitting and resampling
```{r}
set.seed(123)
hotel_split <- initial_split(hotels, strata = children,
                             prop = 0.75)
hotel_other <- training(hotel_split)
hotel_test <- testing(hotel_split)
```


```{r}
# create validation set
set.seed(234)
val_set <- validation_split(hotel_other, strata = children,
                            prop = 0.8)
```

## penalized glm model (lasso)

Set up model
```{r}
lr_mod <- logistic_reg(penalty = tune(),
                       mixture = 1) |>
  set_engine("glmnet")
```

Set up recipe
```{r}
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- recipe(children ~ ., data = hotel_other) |>
  step_date(arrival_date) |>
  step_holiday(arrival_date, holidays = holidays) |>
  step_rm(arrival_date) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors()) |>
  step_normalize(all_predictors())
```

Create workflow
```{r}
lr_workflow <- workflow() |>
  add_model(lr_mod) |>
  add_recipe(lr_recipe)
```

Create tuning grid
```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4,-1,length.out = 30))
```

Train and tune model
```{r}
lr_res <- lr_workflow |>
  tune_grid(val_set, # data
            grid = lr_reg_grid, # tuning param
            control = control_grid(save_pred = T), # save out of sample predictions/ validation data
            metrics = metric_set(roc_auc)) # how to evaluate model performance
```

Now we can visualize
```{r}
lr_res |>
  collect_metrics() |>
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + geom_line() + 
  scale_x_log10(labels = scales::label_number())
```

```{r}
lr_res |>
  show_best(metric = "roc_auc",n = 15) |>
  arrange(-mean,-penalty)
```

All else equal, we probably want a higher penalty value, since it may be less overfit. but we can just autoselect the best one
```{r}
lr_best <- lr_res |>
  collect_metrics() |>
  arrange(-mean)  |>
  slice(1)
```

```{r}
lr_auc <- lr_res  |>
  collect_predictions(parameters = lr_best) |>
  roc_curve(children, .pred_children) |>
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```

## Tree based ensemble
Here, processing could take a while, so we may take advantage of parallel processing
```{r}
cores <- parallel::detectCores()
cores
```

Define model
```{r}
rf_mod <- rand_forest(mtry = tune(),
                      min_n = tune(),
                      trees = 1000 ) |>
  set_engine("ranger",num.threads = cores) |>
  set_mode("classification")
```

Recipe
```{r}
rf_recipe <- recipe(children~ ., data = hotel_other) |>
  step_date(arrival_date) |>
  step_holiday(arrival_date) |>
  step_rm(arrival_date)
```

Workflow
```{r}
rf_workflow <- workflow() |>
  add_model(rf_mod) |>
  add_recipe(rf_recipe)
```

Tune model

```{r}
set.seed(345)
rf_res <- rf_workflow |>
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = T),
            metrics = metric_set(roc_auc))
```

best models
```{r}
rf_res |>
  show_best(metric = "roc_auc")
```

Can already see that AUC is much higher here.
```{r}
autoplot(rf_res)
```

Select best model
```{r}
rf_best <- rf_res |>
  select_best(metric = "roc_auc")

rf_best
```

```{r}
rf_auc <- rf_res |>
  collect_predictions(parameters = rf_best) |>
  roc_curve(children, .pred_children) |>
  mutate(model = "Random forest")
```

```{r}
bind_rows(rf_auc, lr_auc) |>
  ggplot(aes(x = 1-specificity,y = sensitivity, color = model)) + 
  geom_line() + 
  geom_abline(lty = 3) + 
  coord_equal() +
  scale_color_viridis_d(option = "plasma", end = 0.6)
```

## fit final model
```{r}
last_rf_mod <- rand_forest(mtry = 9, min_n = 3, trees = 1000) |>
  set_engine("ranger",num.threads = cores, importance = "impurity") |>
  set_mode("classification")

last_rf_workflow <- rf_workflow |>
  update_model(last_rf_mod) 

set.seed(345)
last_rf_fit <- last_rf_workflow |>
  last_fit(hotel_split)
```


```{r}
last_rf_fit |>
  collect_metrics()
```

Pretty high AUC at 0.923

Check variable importance:

```{r}
last_rf_fit |>
  extract_fit_parsnip() |>
  vip(num_features = 20)
```

```{r}
last_rf_fit |>
  collect_predictions() |>
  roc_curve(truth = children,.pred_children) |>
  autoplot()
```

